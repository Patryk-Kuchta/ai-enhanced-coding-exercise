INFERENCE_SERVER_URL=http://localhost:1234/v1
MODEL_NAME=llama-3.2-1b-instruct
OPENAI_API_KEY=your_api_key_here
